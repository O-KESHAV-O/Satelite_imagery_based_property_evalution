{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6416028a-093a-403e-86d0-d6460c2b5f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "data = joblib.load('ml_data.pkl')\n",
    "\n",
    "X_train = data['X_train']\n",
    "X_test  = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test  = data['y_test']\n",
    "X_submit = data['X_submit']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "486bafbb-70ba-4894-adad-5bc85221d44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Parameters: {'knn__metric': 'manhattan', 'knn__n_neighbors': 11, 'knn__weights': 'distance'}\n",
      "Best CV RMSE: 0.2145274798848337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\envs\\test1\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:131: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\envs\\test1\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 247, in _count_physical_cores\n",
      "    cpu_count_physical = _count_physical_cores_win32()\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\envs\\test1\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 299, in _count_physical_cores_win32\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\envs\\test1\\Lib\\subprocess.py\", line 554, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\envs\\test1\\Lib\\subprocess.py\", line 1039, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ASUS\\anaconda3\\envs\\test1\\Lib\\subprocess.py\", line 1554, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.211193976765885\n",
      "Test R2 Score: 0.8383689098327826\n"
     ]
    }
   ],
   "source": [
    "# KNN Regression with GridSearchCV (FULL CODE)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsRegressor())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'knn__n_neighbors': [3, 5, 7, 9, 11, 15],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',  # RMSE\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_knn = grid.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best CV RMSE:\", -grid.best_score_)\n",
    "\n",
    "y_pred = best_knn.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test RMSE:\", rmse)\n",
    "print(\"Test R2 Score:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05e5545e-b13e-43ac-9ee0-95f09943d38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n",
      "Best Parameters: {'max_depth': 15, 'max_features': None, 'min_samples_leaf': 5, 'min_samples_split': 30}\n",
      "Best CV RMSE: 0.2168443154342746\n",
      "Test RMSE: 0.2162636534336514\n",
      "Test R2 Score: 0.8305159173483874\n",
      "Train RMSE: 0.15742318314556503\n",
      "RMSE Gap (Train - Test): -0.058840470288086366\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regression with GridSearchCV (FULL)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, 15, None],\n",
    "    'min_samples_split': [2, 10, 30, 50],\n",
    "    'min_samples_leaf': [1, 5, 10, 20],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    estimator=dt,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',  # RMSE\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_dt = grid.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best CV RMSE:\", -grid.best_score_)\n",
    "\n",
    "y_pred = best_dt.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test RMSE:\", rmse)\n",
    "print(\"Test R2 Score:\", r2)\n",
    "\n",
    "train_pred = best_dt.predict(X_train)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "\n",
    "print(\"Train RMSE:\", train_rmse)\n",
    "print(\"RMSE Gap (Train - Test):\", train_rmse - rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "914c575c-74de-4e95-9b82-376dade263fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n",
      "Best Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 400}\n",
      "Best CV RMSE: 0.18287569905894435\n",
      "Test RMSE: 0.18030324473294768\n",
      "Test R2 Score: 0.8821935874932131\n",
      "Train RMSE: 0.06933388417663111\n",
      "RMSE Gap (Train - Test): -0.11096936055631657\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Regression with GridSearchCV (FULL)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 400],\n",
    "    'max_depth': [None,5, 10, 20, 30],\n",
    "    'min_samples_split': [2, 10, 30],\n",
    "    'min_samples_leaf': [1, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',  # RMSE\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best CV RMSE:\", -grid.best_score_)\n",
    "\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluate on test set\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test RMSE:\", rmse)\n",
    "print(\"Test R2 Score:\", r2)\n",
    "\n",
    "train_pred = best_rf.predict(X_train)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "\n",
    "print(\"Train RMSE:\", train_rmse)\n",
    "print(\"RMSE Gap (Train - Test):\", train_rmse - rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f43061b-54c4-49d3-be93-56b40a18c6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE (log-price): 0.2485068803400851\n",
      "Linear Regression R2 Score: 0.7762110281488033\n",
      "Linear Regression RMSE (actual price): 176161.35042429125\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression - Full Code\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_log = lr_model.predict(X_test)\n",
    "\n",
    "#  Evaluate on log scale\n",
    "rmse_log = np.sqrt(mean_squared_error(y_test, y_pred_log))\n",
    "r2 = r2_score(y_test, y_pred_log)\n",
    "\n",
    "print(\"Linear Regression RMSE (log-price):\", rmse_log)\n",
    "print(\"Linear Regression R2 Score:\", r2)\n",
    "\n",
    "#  Convert predictions back to actual price\n",
    "y_test_pred = np.expm1(y_pred_log)\n",
    "y_test_true = np.expm1(y_test)\n",
    "\n",
    "rmse_price = np.sqrt(mean_squared_error(y_test_true, y_test_pred))\n",
    "\n",
    "print(\"Linear Regression RMSE (actual price):\", rmse_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f49bc866-d61c-475c-a1fa-8bfc7b70853c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 500, 'subsample': 0.8}\n",
      "Best CV RMSE (log-price): 0.1656083904070192\n",
      "Test RMSE (log-price): 0.1640085632274159\n",
      "Test R2 Score: 0.9025246295710806\n",
      "Test RMSE (actual price): 115944.18830360653\n",
      "Train RMSE (log-price): 0.12601334878469334\n",
      "RMSE Gap (Train - Test): -0.03799521444272255\n"
     ]
    }
   ],
   "source": [
    "# XGBoost Regression with GridSearchCV (FULL)\n",
    "\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [300, 500],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',  # RMSE on log(price)\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_xgb = grid.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best CV RMSE (log-price):\", -grid.best_score_)\n",
    "\n",
    " # Predict on test data (log scale)\n",
    "y_pred_log = best_xgb.predict(X_test)\n",
    "\n",
    "# Evaluate on log scale\n",
    "rmse_log = np.sqrt(mean_squared_error(y_test, y_pred_log))\n",
    "r2 = r2_score(y_test, y_pred_log)\n",
    "\n",
    "print(\"Test RMSE (log-price):\", rmse_log)\n",
    "print(\"Test R2 Score:\", r2)\n",
    "\n",
    "# Convert back to actual price\n",
    "y_test_pred = np.expm1(y_pred_log)\n",
    "y_test_true = np.expm1(y_test)\n",
    "\n",
    "rmse_price = np.sqrt(mean_squared_error(y_test_true, y_test_pred))\n",
    "print(\"Test RMSE (actual price):\", rmse_price)\n",
    "\n",
    "# Overfitting check\n",
    "train_pred_log = best_xgb.predict(X_train)\n",
    "train_rmse_log = np.sqrt(mean_squared_error(y_train, train_pred_log))\n",
    "\n",
    "print(\"Train RMSE (log-price):\", train_rmse_log)\n",
    "print(\"RMSE Gap (Train - Test):\", train_rmse_log - rmse_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c608462c-e90b-432a-ba80-7ed24ee0e1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1805\n",
      "[LightGBM] [Info] Number of data points in the train set: 12967, number of used features: 16\n",
      "[LightGBM] [Info] Start training from score 13.043959\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.03, 'max_depth': 7, 'n_estimators': 700, 'num_leaves': 31, 'subsample': 0.8}\n",
      "Best CV RMSE (log-price): 0.16354761582130012\n",
      "Test RMSE (log-price): 0.16274975480461298\n",
      "Test R2 Score: 0.9040151850258936\n",
      "Test RMSE (actual price): 116674.26292044243\n",
      "Train RMSE (log-price): 0.1264557348683431\n",
      "RMSE Gap (Train - Test): -0.03629401993626988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\envs\\test1\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\anaconda3\\envs\\test1\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LightGBM Regression with GridSearchCV (FULL)\n",
    "\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "lgbm = lgb.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [300, 500, 700],\n",
    "    'learning_rate': [0.03, 0.05, 0.1],\n",
    "    'max_depth': [-1, 5, 7],\n",
    "    'num_leaves': [31, 50, 70],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    " # GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    estimator=lgbm,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',  # RMSE on log(price)\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_lgbm = grid.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best CV RMSE (log-price):\", -grid.best_score_)\n",
    "\n",
    "# 6. Predict on test data (log scale)\n",
    "y_pred_log = best_lgbm.predict(X_test)\n",
    "\n",
    "# 7. Evaluate on log scale\n",
    "rmse_log = np.sqrt(mean_squared_error(y_test, y_pred_log))\n",
    "r2 = r2_score(y_test, y_pred_log)\n",
    "\n",
    "print(\"Test RMSE (log-price):\", rmse_log)\n",
    "print(\"Test R2 Score:\", r2)\n",
    "\n",
    "# 8. Convert back to actual price\n",
    "y_test_pred = np.expm1(y_pred_log)\n",
    "y_test_true = np.expm1(y_test)\n",
    "\n",
    "rmse_price = np.sqrt(mean_squared_error(y_test_true, y_test_pred))\n",
    "print(\"Test RMSE (actual price):\", rmse_price)\n",
    "\n",
    "# 9. Overfitting check\n",
    "train_pred_log = best_lgbm.predict(X_train)\n",
    "train_rmse_log = np.sqrt(mean_squared_error(y_train, train_pred_log))\n",
    "\n",
    "print(\"Train RMSE (log-price):\", train_rmse_log)\n",
    "print(\"RMSE Gap (Train - Test):\", train_rmse_log - rmse_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95627038-08d8-4dde-84d8-c792911d8bd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mX_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'info'"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f56bbff-a163-4f9b-8b5f-b81ebae8076f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dfb117-fbe3-40c3-b24f-08b425657268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287263d-b012-413f-8139-041c67981d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a87f2c6-1f95-42ca-9342-c59d515115ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f40e3b-a025-484e-a3e2-b6efcaa7117b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53eb55-7c5b-4bdd-9015-906fd612d4c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6205a9ce-71bd-41d1-b081-dbfb0355d5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ad85fd-62b2-495d-9e31-82c8cd2ae104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bfb576-5f6e-4fdf-8ecd-dab3be4a92bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
